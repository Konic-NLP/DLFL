{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt-wGHqVL10F"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOm_EYQ2L10I"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYK-XKuDL10J"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOHBBOGCL10K"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9huvL9L10L"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-JI3vr_L10L"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OAkti-6L10M"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOk-3cNL10N"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ZmMSbtL10O",
        "outputId": "2c03e40c-f70a-49a9-b788-0ad0e8ba8092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-28 20:43:42--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  37.7MB/s    in 76s     \n",
            "\n",
            "2022-07-28 20:44:58 (32.7 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZVYAgC9jL10Q"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ps1N-5MjL10R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9544ed-b7e1-4143-f08f-00326e90bac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-28 20:49:20--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  37.1MB/s    in 60s     \n",
            "\n",
            "2022-07-28 20:50:21 (33.4 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XQYhFqmL10R"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zFIUcKfjL10S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417b09e0-32ac-4fd8-d2b2-8184bcc8640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy1D0Ei9L10S"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC_EYWBvL10T"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "It93c-AsL10T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f54a03-3c37-442f-9a85-71b62b5af67c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0c5b84b430>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bpU1Pvz7L10U"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr2_KMUBL10U"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yicHs3zL10U"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlfWAMbuL10V"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aDAcyIf3L10V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129a077d-4b50-4676-b68f-4565027a306c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\".\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXpq6AOOL10W"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y2PbYnqYL10W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971c024c-4394-425e-b847-d3cc2a8ad4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67E8-rWIL10X"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VRA9g5ML10X"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "iAIFjD3ctgvb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR8EFMmQL10X"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jNx2h4FCL10X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cba61a-5100-4205-cd21-7739b25463b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model=models.resnet18(pretrained=True)\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aXwtOrsL10Y"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lPHjTJMPL10Y"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUj-SM9gL10Y"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpFqlDzgL10Y"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MKVG12MFL10Z"
      },
      "outputs": [],
      "source": [
        "model.fc=nn.Linear(512,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgswGb3L10Z"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ywlatot-L10Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9342ac-be53-434b-ed44-fc274ae5708b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAilw9WkL10Z"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMeVclNL10a"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpFprOyRL10a"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX-9SOnxt7By",
        "outputId": "91e1b0e3-a4dd-4daf-cd7a-bddcc8ac7678"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QsQJi4urL10a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKG2_Ou9L10b"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aQr_y1kML10b"
      },
      "outputs": [],
      "source": [
        "batch_size=100\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size)\n",
        "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13z-wh8-L10b"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JJNbr_U_L10b"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEETvfumL10b"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XWWn2MWL10c"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bpI95R_-L10c"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "       \n",
        "        model.train() \n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z=model(x)\n",
        "        # calculate loss \n",
        "        loss=criterion(z,y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        x_test,y_test=x_test.to(device),y_test.to(device)\n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z=model(x_test)\n",
        "        _,yhat=torch.max(z.data,1)\n",
        "        #find max \n",
        "       \n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct+=(yhat==y_test).sum().item()\n",
        "      \n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YS8w01hL10c"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DL_cYH4_L10d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bef70e-7daa-459a-8613-6c813ee3b280"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9942"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Hj7j_cLDL10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "16796c64-1688-4ee2-f708-f004a10e4112"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot([i.cpu() for i in loss_list])\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253YbxwVL10d"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsYivJoLL10d"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "_fUcK1EqL10e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4716d2-25cc-4784-cd20-ea6656745d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample : 2; Expected Label: tensor(1, device='cuda:0'); Obtained Label: tensor(0, device='cuda:0')\n",
            "Sample : 98; Expected Label: tensor(1, device='cuda:0'); Obtained Label: tensor(0, device='cuda:0')\n",
            "Sample : 24; Expected Label: tensor(1, device='cuda:0'); Obtained Label: tensor(0, device='cuda:0')\n",
            "Sample : 56; Expected Label: tensor(1, device='cuda:0'); Obtained Label: tensor(0, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "max_num_of_items = 4  # first four mis-classified samples\n",
        "# validation_loader_batch_one = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "\n",
        "for i, (x_test, y_test) in enumerate(validation_loader):\n",
        "    # set model to eval\n",
        "    model.eval()\n",
        "    x_test,y_test=x_test.to(device), y_test.to(device)\n",
        "    # make a prediction\n",
        "    z = model(x_test)\n",
        "    \n",
        "    # find max\n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    \n",
        "    # print mis-classified samples\n",
        "    for j  in range(len(yhat)):\n",
        "      if yhat[j] != y_test[j]:\n",
        "          print(\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(j), str(y_test[j]), str(yhat[j])))\n",
        "          count+=1\n",
        "      if count==max_num_of_items:\n",
        "        break\n",
        "    \n",
        "    if count==max_num_of_items:\n",
        "        break\n",
        "    # end if           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH-zqN7DL10e"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo3nSdBNL10e"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAPLQL27L10e"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAXhEKBJL10f"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4.1_resnet18_PyTorch_Sijia.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}